# kangur-ai-evaluation
Testing Multimodal large language models (MLLMs) for High School level mathematical reasoning.

This repository contains the dataset and code for evaluating Multimodal Large Language Models (MLLMs) on high-school-level mathematical reasoning in four languages (Spanish, Catalan, French, English), using questions from the Kangaroo Mathematics Competition (https://cangur.alcoi.upv.es/edicions.php). The dataset is formatted as an Excel file with questions, multiple-choice options, ground truth answers, presence of a figure, and scores (dataset)

It includes the scripts to access models such as GPT, Gemini, Llama, and ALIA (codes).

Experiments were conducted via API services and local/cloud instances for different model sizes. The raw results, as well as an updated Excel file with the response of each model and its reasoning process, are available, too (raw_responses).
